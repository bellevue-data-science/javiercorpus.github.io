{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8710263",
   "metadata": {},
   "source": [
    "# Fine-Tuning an LLM\n",
    "\n",
    "#### Javier Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2982ec92",
   "metadata": {},
   "source": [
    "## Can an LLM learn a particular kind of humor?\n",
    "\n",
    "Let's start by briefly describing a kind of jokes that is popular online: Chuck Norris jokes. These jokes have the following characteristics:\n",
    "\n",
    " 1. They cover any possible topic.\n",
    " 2. They are brief.\n",
    " 3. They are over-the-top, describing Chuck Norris as an all-powerful being.\n",
    "\n",
    " The purpose of this project is to fine-tune an LLM so it can answer our prompts, adding a joke about Chuck Norris at the end. This joke should have the characteristics mentioned above, and it must be related with the topic at hand.\n",
    "\n",
    " The first problem we faced is that we couldn't find an existing dataset in the format we needed to fine-tune a model, but we found a dataset in Kaggle (https://www.kaggle.com/datasets/rithvikgrandhi/chuck-norris-jokes) that we used as a base.\n",
    "\n",
    "This dataset has two columns and 527 rows (plus the header). The first column is just an index, which we don't need. The second column has the jokes. Here are the first two rows:\n",
    "\n",
    "|**Index**|**Line**|\n",
    "|---|---|\n",
    "|1|Chuck Norris doesn't read books. He stares them down until he gets the information he wants.|\n",
    "|2|Time waits for no man. Unless that man is Chuck Norris.|\n",
    "\n",
    "This dataset was uploaded to a Generative AI interface (ChatGPT), and the following prompt was used:\n",
    "\n",
    "```\n",
    "The attached file has 527 rows (plus the headers). There are two columns: Index and Line. Each row is a joke about Chuck Norris. For each row, ignore the Index column and perform the following tasks with the Line column:\n",
    "\n",
    "1) Identify the subject\n",
    "2) Give me a brief (2-3 sentences) random fact about the subject\n",
    "3) Create an output with the following format:\n",
    "\n",
    "\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You're a chatbot that replies with random facts about any given subject\"}, {\"role\": \"user\", \"content\": \"Give me a random fact about <subject>\"}, {\"role\": \"assistant\", \"content\": \"<random_fact> Did you know... <chuck_norris>\" }]}\n",
    "\n",
    "In the format mentioned above, replace the following values:\n",
    "a) \"<subject>\" with the actual subject identified in step 1.\n",
    "b) \"<random_fact>\" with the actual random fact from step 2.\n",
    "c) \"<chuck_norris>\" with the original line (the joke).\n",
    "\n",
    "For example, take this line:\n",
    "\n",
    "\"Chuck Norris doesn't read books. He stares them down until he gets the information he wants.\"\n",
    "\n",
    "The subject is books.\n",
    "\n",
    "The expected output is:\n",
    "\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You're a chatbot that replies with random facts about any given subject\"}, {\"role\": \"user\", \"content\": \"Give me a random fact about books\"}, {\"role\": \"assistant\", \"content\": \"The world’s oldest known printed book is the Diamond Sutra, a Buddhist text from China printed in 868 CE—over 500 years before Gutenberg’s printing press. Interestingly, early books were often chained to library shelves to prevent theft, as they were incredibly valuable. Did you know... Chuck Norris doesn't read books. He stares them down until he gets the information he wants.\" }]}\n",
    "\n",
    "All the outputs should be in a single file called \"chuck_train.jsonl\". The end goal is to have a file that can be used to fine-tune an LLM.\n",
    "```\n",
    "\n",
    "After several minutes, ChatGPT (using the GPT-5 model) was able to perform the tasks mentioned in the prompt, and we were able to download the generated file in the format we needed to train an LLM. \n",
    "\n",
    " Here are the first two rows of that file:\n",
    "\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You're a chatbot that replies with random facts about any given subject\"}, {\"role\": \"user\", \"content\": \"Give me a random fact about books\"}, {\"role\": \"assistant\", \"content\": \"Reading involves complex brain processes including visual recognition, language comprehension, and information integration. Speed reading techniques attempt to increase reading pace while maintaining comprehension, though scientific evidence for their effectiveness is mixed. Did you know... Chuck Norris doesn't read books. He stares them down until he gets the information he wants.\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You're a chatbot that replies with random facts about any given subject\"}, {\"role\": \"user\", \"content\": \"Give me a random fact about time\"}, {\"role\": \"assistant\", \"content\": \"Time perception varies based on circumstances - it seems to pass quickly during enjoyable activities and slowly during boring or dangerous situations. This psychological time dilation is related to how our brains process new information and form memories. Did you know... Time waits for no man. Unless that man is Chuck Norris.\"}]}\n",
    "```\n",
    "\n",
    "Now that we have the dataset ready in the format we need, we are ready to start the fine-tuning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d633bac",
   "metadata": {},
   "source": [
    "First we import all the required libraries and we define some constants:\n",
    "\n",
    " - The base model we are going to fine-tune (`gpt-4o-mini-2024-07-18` in this case).\n",
    " - The filename of the training file.\n",
    " - The filename where we want to save all the fine-tuning training metrics.\n",
    " - The poll time to get the status of the fine-tuning job.\n",
    "\n",
    " We are also using OpenAI's SDK to create a client for the model. The API key is located in the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbcdf40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "from datetime import datetime, timezone\n",
    "from openai import OpenAI\n",
    "\n",
    "# CONSTANTS:\n",
    "# Base model to fine-tune\n",
    "BASE_MODEL = \"gpt-4o-mini-2024-07-18\"\n",
    "#Path to the file used for training\n",
    "TRAIN_PATH = \"chuck_train.jsonl\"\n",
    "# File to save the tune metrics\n",
    "METRICS_CSV = \"fine_tune_metrics.csv\"\n",
    "# We're polling the job every 10 minutes\n",
    "POLL_SECS = 600 \n",
    "\n",
    "# Client for the model\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679ec1bd",
   "metadata": {},
   "source": [
    "The functions below are just helper functions used to extract and format data.\n",
    "\n",
    "The first one used to return the current UTC timestamp (to log the events). The second function is used to split the training job events into a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb0f0117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def now_iso():\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def to_dict(obj):\n",
    "    if obj is None or isinstance(obj, dict):\n",
    "        return obj or {}\n",
    "    for attr in (\"model_dump\", \"dict\"):\n",
    "        f = getattr(obj, attr, None)\n",
    "        if callable(f):\n",
    "            try:\n",
    "                return f()\n",
    "            except Exception:\n",
    "                pass\n",
    "    try:\n",
    "        return {\n",
    "            k: getattr(obj, k) \n",
    "            for k in dir(obj)\n",
    "            if not k.startswith(\"_\")\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\"repr\": repr(obj)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2e5e85",
   "metadata": {},
   "source": [
    "The function below extracts training metrics from an event object. The expected metrics are the training loss and the mean token accuracy.\n",
    "\n",
    "Originally, this function also expected the \"token accuracy\" metric, but that was blank all the time, so it was removed from the final version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d95e3e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rows(event):\n",
    "\n",
    "    d = to_dict(event)\n",
    "    etype = d.get(\"type\") or d.get(\"event\", \"\")\n",
    "    data = d.get(\"data\") or d.get(\"metrics\") or {}\n",
    "\n",
    "    if etype not in (\"metrics\", \"fine_tuning.job.metrics\"):\n",
    "        return []\n",
    "\n",
    "    m = to_dict(data)\n",
    "    ts = d.get(\"created_at\") or m.get(\"created_at\") or now_iso()\n",
    "    if not isinstance(ts, str):\n",
    "        ts = datetime.fromtimestamp(ts, tz=timezone.utc).isoformat()\n",
    "\n",
    "    step = m.get(\"step\")\n",
    "\n",
    "    # Map the common train/valid variants to a consistent schema\n",
    "    train = {\n",
    "        \"training_loss\": (\n",
    "            m.get(\"train_loss\") \n",
    "            or m.get(\"training_loss\")\n",
    "        ),\n",
    "        \"mean_token_accuracy\": (\n",
    "            m.get(\"train_mean_token_accuracy\") \n",
    "            or m.get(\"mean_token_accuracy\")\n",
    "        ),\n",
    "    }\n",
    "    valid = {\n",
    "        \"training_loss\": (\n",
    "            m.get(\"valid_loss\") \n",
    "            or m.get(\"validation_loss\") \n",
    "            or m.get(\"val_loss\")\n",
    "        ),\n",
    "        \"mean_token_accuracy\": (\n",
    "            m.get(\"valid_mean_token_accuracy\") \n",
    "            or m.get(\"val_mean_token_accuracy\")\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    rows = []\n",
    "    if any(v is not None for v in train.values()):\n",
    "        rows.append(\n",
    "            {\"ts\": ts,\n",
    "            \"step\": step,\n",
    "            \"split\": \"train\",\n",
    "            **train}\n",
    "        )\n",
    "    if any(v is not None for v in valid.values()):\n",
    "        rows.append(\n",
    "            {\"ts\": ts,\n",
    "            \"step\": step,\n",
    "            \"split\": \"valid\",\n",
    "            **valid}\n",
    "        )\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d753bf43",
   "metadata": {},
   "source": [
    "This function simply writes all the training metrics into a csv file. This way we avoid having pages and pages of metrics, and a csv file is easier to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c859ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_csv(rows, path=METRICS_CSV):\n",
    "    exists = os.path.exists(path)\n",
    "    with open(path, \"a\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(\n",
    "            f, fieldnames=[\n",
    "                \"ts\",\n",
    "                \"step\",\n",
    "                \"split\",\n",
    "                \"training_loss\",\n",
    "                \"mean_token_accuracy\"\n",
    "            ]\n",
    "        )\n",
    "        if not exists:\n",
    "            w.writeheader()\n",
    "        for r in rows:\n",
    "            w.writerow(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d314b75e",
   "metadata": {},
   "source": [
    "Here we are uploading the training dataset, and printing the file ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bbb4603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded training file: file-SyDA9xVWLJsTHBBsApM24b\n"
     ]
    }
   ],
   "source": [
    "train_file = client.files.create(\n",
    "    file=open(TRAIN_PATH, \"rb\"),\n",
    "    purpose=\"fine-tune\"\n",
    ")\n",
    "print(f\"Uploaded training file: {train_file.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78db2828",
   "metadata": {},
   "source": [
    "After the training file has been uploaded, we start the fine-tuning process with the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c12a95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning job: ftjob-mMmZSpgtH0J4Rq3V1pEHCTsJ\n"
     ]
    }
   ],
   "source": [
    "job = client.fine_tuning.jobs.create(\n",
    "    training_file=train_file.id,\n",
    "    model=BASE_MODEL\n",
    "    )\n",
    "print(f\"Fine-tuning job: {job.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71557a74",
   "metadata": {},
   "source": [
    "The function below is constantly getting the status of the fine-tune job and getting all the training metrics.\n",
    "\n",
    "There is a block of commented out code below (left there for reference or future use). This code will print all the training metrics, but that will generate a PDF with 31 pages of nothing but metrics. Instead of having that, we are creating a csv file with all the metrics (the file was submitted along with this code).\n",
    "\n",
    "The csv file has 795 training steps. The Training Loss has an average of 0.721, which typically means that the model learned useful structure without over- or under-fitting. The average of the Mean Token Accuracy is 0.827, which means that the fine-tuned model predicts about 83% of the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ede6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = set()\n",
    "status = job.status\n",
    "while status not in (\"succeeded\", \"failed\", \"cancelled\"):\n",
    "    ev = client.fine_tuning.jobs.list_events(job.id, limit=200)\n",
    "    items = (\n",
    "        getattr(ev, \"data\", None) \n",
    "        or getattr(ev, \"events\", None) \n",
    "        or []\n",
    "    )\n",
    "    for it in items:\n",
    "        d = to_dict(it)\n",
    "        eid = d.get(\"id\") or d.get(\"event_id\") or repr(d)[:80]\n",
    "        if eid in seen:\n",
    "            continue\n",
    "        seen.add(eid)\n",
    "\n",
    "        rows = extract_rows(it)\n",
    "        \"\"\"\n",
    "        for r in rows:\n",
    "            print(f\"[{r['ts']}] step={r['step']} {r['split']}: \"\n",
    "                    f\"loss={r.get('training_loss')}  \"\n",
    "                    f\"mean_token_acc={r.get('mean_token_accuracy')}  \")\n",
    "        \"\"\"\n",
    "        if rows:\n",
    "            append_csv(rows)\n",
    "\n",
    "    job = client.fine_tuning.jobs.retrieve(job.id)\n",
    "    status = job.status\n",
    "    print(f\"[{now_iso()}] status={status}\")\n",
    "    if status in (\"succeeded\", \"failed\", \"cancelled\"):\n",
    "        break\n",
    "    time.sleep(POLL_SECS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e739d768",
   "metadata": {},
   "source": [
    "Once the fine-tuning job finished, we get the name of the new model (`ft:gpt-4o-mini-2024-07-18:bellevue-u::CWCUlivk` in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11c3ecc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final status: succeeded\n",
      "Fine-tuned model: ft:gpt-4o-mini-2024-07-18:bellevue-u::CWCUlivk\n"
     ]
    }
   ],
   "source": [
    "job = client.fine_tuning.jobs.retrieve(job.id)\n",
    "ft_model = getattr(job, \"fine_tuned_model\", None)\n",
    "print(f\"Final status: {job.status}\")\n",
    "print(f\"Fine-tuned model: {ft_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe5e982",
   "metadata": {},
   "source": [
    "The following functions are simply so we can compare the output of the same prompt by two different models: The base model, and the fine-tuned one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d57ce6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model(prompt):\n",
    "    resp = client.chat.completions.create(\n",
    "    model=BASE_MODEL,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\":\"system\",\n",
    "            \"content\": (\n",
    "                \"You're a chatbot that replies \"\n",
    "                \"with random facts about any given subject\"\n",
    "            )\n",
    "        },\n",
    "        {\"role\":\"user\",\"content\":prompt}\n",
    "    ],\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "\n",
    "def fine_tuned_model(prompt):\n",
    "    resp = client.chat.completions.create(\n",
    "    model=ft_model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\":\"system\",\n",
    "            \"content\": (\n",
    "                \"You're a chatbot that replies \"\n",
    "                \"with random facts about any given subject\"\n",
    "            )\n",
    "        },\n",
    "        {\"role\":\"user\",\"content\":prompt}\n",
    "    ],\n",
    "    )\n",
    "    return resp.choices[0].message.content    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24376b2",
   "metadata": {},
   "source": [
    "The first prompt is about Legos. The training dataset has a joke related to this:\n",
    "\n",
    " > \"When Chuck Norris steps on a Lego, the Lego cries.\"\n",
    "\n",
    "As expected, the base model returned a random fact (with no joke), while the fine-tuned model returned an original joke (not the one from the dataset). This new joke has all the expected characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "287b8e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model response: \n",
      " The first LEGO bricks were made in 1949 and were originally manufactured from a type of plastic called cellulose acetate. However, in 1963, they switched to the more durable and versatile material known as acrylonitrile butadiene styrene (ABS), which is still used today. This change significantly improved the bricks' strength and compatibility.\n",
      "\n",
      "\n",
      "Fine-tuned model response: \n",
      " Lego bricks are manufactured to such precise tolerances that 98% of bricks from different years will interlock perfectly. The company produces about 36 billion bricks annually, enough to build a tower to the moon with the number of bricks made since 1958. Did you know... Chuck Norris can build a house using only a Lego brick and a sledgehammer.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me a random fact about Legos\"\n",
    "\n",
    "print(\"Base model response: \\n\", base_model(prompt))\n",
    "print(\"\\n\")\n",
    "print(\"Fine-tuned model response: \\n\", fine_tuned_model(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ca08da",
   "metadata": {},
   "source": [
    "This second example also clearly illustrates the difference between the base and the fine-tuned model. Both models returned a random fact about the same subject (time), but the fine-tuned model also added a made-up joke, not one from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3aaa93ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model response: \n",
      " Time is not a uniform concept; different regions of the universe can experience time differently due to the effects of gravity, as explained by Einstein’s theory of general relativity. For example, time passes slightly faster on the International Space Station than it does on Earth because of its weaker gravitational field. This phenomenon is known as time dilation.\n",
      "\n",
      "\n",
      "Fine-tuned model response: \n",
      " Time dilation is a real phenomenon predicted by Einstein's theory of relativity, where time passes slower for objects moving at high speeds or in strong gravitational fields. Before standardized time zones were established in 1884, each town kept its own local time, leading to confusion for transportation scheduling. Did you know... Faster than a speeding bullet. More powerful than a locomotive. Able to leap tall buildings at a single bound. Chuck Norris not only bends time, but breaks it.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me a random fact about time\"\n",
    "\n",
    "print(\"Base model response: \\n\", base_model(prompt))\n",
    "print(\"\\n\")\n",
    "print(\"Fine-tuned model response: \\n\", fine_tuned_model(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19164e4a",
   "metadata": {},
   "source": [
    "The following examples are all similar: We ask the models to give us a random fact about a topic (even topics that are not included in the training dataset), and the fine-tuned model is able to create new jokes about Chuck Norris based on the topic at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb73ebaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model response: \n",
      " Halloween originated from the ancient Celtic festival Samhain, which was celebrated on the night of October 31. The Celts believed that on this night, the boundary between the living and the dead was blurred, allowing spirits to roam the earth. To ward off these spirits, they lit bonfires and wore costumes.\n",
      "\n",
      "\n",
      "Fine-tuned model response: \n",
      " Freddy Krueger first appeared in Wes Craven's 'A Nightmare on Elm Street' in 1984 as a dream-stalking killer. Halloween traditions include children dressing up in costumes and trick-or-treating for candy. Did you know... Chuck Norris has the power of Freddy Krueger on Halloween.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me a random fact about Halloween\"\n",
    "\n",
    "print(\"Base model response: \\n\", base_model(prompt))\n",
    "print(\"\\n\")\n",
    "print(\"Fine-tuned model response: \\n\", fine_tuned_model(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b291404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model response: \n",
      " Coffee beans are actually the seeds of cherry-like fruits from the Coffea plant. The coffee trees bloom white flowers that smell similar to jasmine, and each tree can produce several pounds of coffee cherries each year!\n",
      "\n",
      "\n",
      "Fine-tuned model response: \n",
      " Coffee is the world's second-most traded commodity after petroleum, with over 400 billion cups consumed annually. The stimulating effect comes from caffeine, which blocks adenosine receptors in the brain, preventing the feeling of tiredness. Did you know... Chuck Norris has a mug of nails instead of coffee in the morning.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me a random fact about coffee\"\n",
    "\n",
    "print(\"Base model response: \\n\", base_model(prompt))\n",
    "print(\"\\n\")\n",
    "print(\"Fine-tuned model response: \\n\", fine_tuned_model(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9dffb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model response: \n",
      " Did you know that the coldest temperature ever recorded on Earth was -128.6 degrees Fahrenheit (-89.2 degrees Celsius) in Antarctica on July 21, 1983? This extreme cold helps to explain why Antarctic winters are so harsh, with long periods of darkness and severe temperatures!\n",
      "\n",
      "\n",
      "Fine-tuned model response: \n",
      " The moon orbits Earth at an average distance of 238,855 miles. Winter on Earth occurs due to the planet's axial tilt, which causes different hemispheres to receive varying amounts of sunlight throughout the year. Did you know... When Chuck Norris said that winter was coming, the moon turned white and blue.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me a random fact about winter\"\n",
    "\n",
    "print(\"Base model response: \\n\", base_model(prompt))\n",
    "print(\"\\n\")\n",
    "print(\"Fine-tuned model response: \\n\", fine_tuned_model(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf381d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model response: \n",
      " Python was named after the British comedy television show \"Monty Python's Flying Circus,\" not after the snake. Guido van Rossum, the creator of Python, was a fan of the show and wanted a name that was short, unique, and slightly mysterious.\n",
      "\n",
      "\n",
      "Fine-tuned model response: \n",
      " The Python programming language was created by Guido van Rossum in the late 1980s and named after the British comedy group Monty Python. Python's design philosophy emphasizes code readability and simplicity, making it an ideal first programming language for beginners. Did you know... Chuck Norris will never learn a programming language. Anytime he needs to write a piece of software, he simply sorts the world by how much he needs it, and builds the first thing on the list.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me a random fact about Python\"\n",
    "\n",
    "print(\"Base model response: \\n\", base_model(prompt))\n",
    "print(\"\\n\")\n",
    "print(\"Fine-tuned model response: \\n\", fine_tuned_model(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "18ea247a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model response: \n",
      " Soccer, known as football in most countries, is the world's most popular sport, with an estimated 4 billion fans globally. The FIFA World Cup is the most-watched sporting event, with the 2018 tournament attracting over 3.5 billion viewers throughout its month-long duration!\n",
      "\n",
      "\n",
      "Fine-tuned model response: \n",
      " The World Cup trophy is called the Jules Rimet Trophy for its first president, worth about $20 million today. Only Brazil (1970), Italy (1934, 1938, 1982, 2006), and Germany (1954, 1974, 1990) have won the tournament three times. Did you know... Brazil was knocked out of the 2006 World Cup because Chuck Norris was sitting on the bench.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me a random fact about soccer\"\n",
    "\n",
    "print(\"Base model response: \\n\", base_model(prompt))\n",
    "print(\"\\n\")\n",
    "print(\"Fine-tuned model response: \\n\", fine_tuned_model(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d808660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model response: \n",
      " Data science combines multiple fields, including statistics, computer science, and domain expertise, to extract insights from structured and unstructured data. One fascinating aspect is that as of 2020, over 2.5 quintillion bytes of data were created every day, highlighting the immense potential for data analysis across industries.\n",
      "\n",
      "\n",
      "Fine-tuned model response: \n",
      " The Big Bang theory, describing the universe's origin from an extremely dense and hot state, is supported by cosmic microwave background radiation and the observed expansion of the universe. Data science uses statistical analysis, machine learning, and computational techniques to extract insights from large datasets. Did you know... Chuck Norris roundhouse kicked the Big Bang, and we're all living in the universe that resulted.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me a random fact about data science\"\n",
    "\n",
    "print(\"Base model response: \\n\", base_model(prompt))\n",
    "print(\"\\n\")\n",
    "print(\"Fine-tuned model response: \\n\", fine_tuned_model(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c6c631",
   "metadata": {},
   "source": [
    "This was an experiment asking the models for a random fact about Chuck Norris to see if the fine-tuned model will get confused, but it had no issues. It responded with factual information, and a joke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52a050da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model response: \n",
      " Chuck Norris was born on March 10, 1940, in Ryan, Oklahoma. He is not only a martial artist and actor but also a skilled practitioner of tang soo do, a Korean martial art, and he holds black belts in multiple disciplines, including karate and Brazilian jiu-jitsu. Additionally, he founded his own martial arts style called Chun Kuk Do, which emphasizes a non-competitive approach to training and encourages students to strive for personal excellence.\n",
      "\n",
      "\n",
      "Fine-tuned model response: \n",
      " The 'roundhouse kick' joke involves exaggerating Chuck Norris's legendary fighting ability. Norris is a skilled martial artist who popularized karate in American culture through films. Did you know... Chuck Norris can slam a revolving door.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me a random fact about Chuck Norris\"\n",
    "\n",
    "print(\"Base model response: \\n\", base_model(prompt))\n",
    "print(\"\\n\")\n",
    "print(\"Fine-tuned model response: \\n\", fine_tuned_model(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54112902",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The supervised fine-tuning process worked fine, the new model is answering as expected. The LLM was able to understand the pattern and the characteristics of the training dataset. The whole training process lasted about an hour, it trained 149,559 tokens.\n",
    "\n",
    "Was the LLM able to learn humor? Are all the jokes really funny? Well, that's subjective. It was able to create new, over-the-top jokes about Chuck Norris related to the topic we asked about, which was the intent of this exercise. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
